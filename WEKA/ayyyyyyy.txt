import java.io.BufferedReader;
import java.io.FileReader;
import java.util.Random;


import weka.classifiers.Evaluation;
import weka.classifiers.bayes.NaiveBayes;
import weka.core.Instances;


public class CombineModels {
	
	public static void main(String[] args) throws Exception {
		//load dataset
		String data = "C:/Users/ergun/Desktop/veriseti.arff";
		DataSource source = new DataSource(data);
		//get instances object 
		Instances trainingData = source.getDataS	et();
		//set class index .. as the last attribute
		if (trainingData.classIndex() == -1) {
			trainingData.setClassIndex(trainingData.numAttributes() - 1);
		}
		
		/* Boosting a weak classifier using the Adaboost M1 method
		 * for boosting a nominal class classifier
		 * Tackles only nominal class problems
		 * Improves performance
		 * Sometimes overfits.
		 */
		//AdaBoost .. 
		AdaBoostM1 m1 = new AdaBoostM1();
		m1.setClassifier(new DecisionStump());//needs one base-classifier
		m1.setNumIterations(20);
		m1.buildClassifier(trainingData);
		
		/* Bagging a classifier to reduce variance.
		 * Can do classification and regression (depending on the base model)
		 */
		//Bagging .. 
		Bagging bagger = new Bagging();
		bagger.setClassifier(new RandomTree());//needs one base-model
		bagger.setNumIterations(25);
		bagger.buildClassifier(trainingData);		
		
		/*
		 * The Stacking method combines several models
		 * Can do classification or regression. 
		 */
		//Stacking ..
		Stacking stacker = new Stacking();
		stacker.setMetaClassifier(new J48());//needs one meta-model
		Classifier[] classifiers = {				
				new J48(),
				new NaiveBayes(),
				new RandomForest()
		};
		stacker.setClassifiers(classifiers);//needs one or more models
		stacker.buildClassifier(trainingData);
		
		/*
		 * Class for combining classifiers.
		 * Different combinations of probability estimates for classification are available. 
		 */
		//Vote .. 
		Vote voter = new Vote();
		voter.setClassifiers(classifiers);//needs one or more classifiers
		voter.buildClassifier(trainingData);
	}
}